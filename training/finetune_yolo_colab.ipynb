{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üéØ YOLO Segmentation Anti-Cheat Model Fine-tuning\n",
        "\n",
        "**IMPORTANT**: This notebook is for training **YOLO Segmentation models** (yolo11s-seg, yolo11x-seg, etc.).\n",
        "\n",
        "This notebook will fine-tune the existing YOLO segmentation model to improve detection of:\n",
        "- üì± **Phone** (currently weak - max ~0.2%)\n",
        "- üìÑ **Material/Paper** (currently weak - max ~0.1%)\n",
        "- üë§ **Person** (needs improvement - max ~5%)\n",
        "- üéß **Headphones** (already good - max ~44%)\n",
        "\n",
        "## Instructions:\n",
        "1. Upload your existing `best.pt` segmentation model to Google Drive\n",
        "2. Run all cells in order\n",
        "3. Download the new `best.onnx` file when done\n",
        "4. Replace `Intelligence-Test/public/models/anticheat_yolo11s.onnx`\n",
        "\n",
        "## Note on Dataset Format:\n",
        "This notebook automatically converts bounding box labels to segmentation polygon format\n",
        "so that detection datasets can be used to train segmentation models."
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mount_drive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install ultralytics -q\n",
        "!pip install onnx onnxruntime -q\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from ultralytics import YOLO\n",
        "\n",
        "print(\"‚úÖ Dependencies installed!\")"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "TARGET_CLASSES = ['person', 'phone', 'material', 'headphones']\n",
        "\n",
        "# Dataset URLs from Roboflow (phone and material focused)\n",
        "DATASETS = [\n",
        "    # Phone datasets (HIGH PRIORITY - model is weak here)\n",
        "    (\"phone_1\", \"https://app.roboflow.com/ds/5ReObgnLbQ?key=HTPSgVzDLW\"),\n",
        "    (\"phone_2\", \"https://app.roboflow.com/ds/f9k54F7Azq?key=eYssUekSYc\"),\n",
        "    \n",
        "    # Paper/Material datasets (HIGH PRIORITY)\n",
        "    (\"paper_1\", \"https://app.roboflow.com/ds/inuabMtp6t?key=jbu7HTlrBf\"),\n",
        "    (\"paper_2\", \"https://app.roboflow.com/ds/b4oxAhlW40?key=4A761Kjm5F\"),\n",
        "    \n",
        "    # Headphones (for balance)\n",
        "    (\"headphones_1\", \"https://app.roboflow.com/ds/qqqEeSKAlk?key=GT1Xa65onI\"),\n",
        "    (\"headphones_2\", \"https://app.roboflow.com/ds/cKHwOqmuda?key=qL10KsWlBt\"),\n",
        "    \n",
        "    # Person\n",
        "    (\"person_1\", \"https://app.roboflow.com/ds/PwRwV0c1jL?key=FgXbXeqlpH\"),\n",
        "]\n",
        "\n",
        "# Class mapping\n",
        "CLASS_MAPPING = {\n",
        "    'person': 'person', 'student': 'person', 'face': 'person', 'head': 'person',\n",
        "    'human': 'person', 'people': 'person', 'man': 'person', 'woman': 'person',\n",
        "    'phone': 'phone', 'mobile': 'phone', 'cell phone': 'phone', \n",
        "    'telephone': 'phone', 'smartphone': 'phone', 'cellphone': 'phone',\n",
        "    'mobile phone': 'phone', 'iphone': 'phone', 'android': 'phone',\n",
        "    'ProductRecog - v2 2024-11-05 7:03am': 'phone',\n",
        "    'paper': 'material', 'document': 'material', 'book': 'material',\n",
        "    'notebook': 'material', 'notes': 'material', 'sheet': 'material',\n",
        "    'material': 'material', 'cheat sheet': 'material', 'PAPER': 'material', 'Paper': 'material',\n",
        "    'headphone': 'headphones', 'headphones': 'headphones', \n",
        "    'earphone': 'headphones', 'earphones': 'headphones',\n",
        "    'headset': 'headphones', 'earbuds': 'headphones', 'earbud': 'headphones',\n",
        "    'airpods': 'headphones', 'ear device': 'headphones', 'Headphone': 'headphones',\n",
        "    'left earbud': 'headphones', 'eardevice': 'headphones',\n",
        "}\n",
        "\n",
        "def normalize_class(class_name):\n",
        "    class_name = class_name.lower().strip()\n",
        "    for key, target in CLASS_MAPPING.items():\n",
        "        if key.lower() == class_name:\n",
        "            return TARGET_CLASSES.index(target)\n",
        "    return -1\n",
        "\n",
        "def bbox_to_segment(bbox_coords):\n",
        "    \"\"\"\n",
        "    Convert YOLO bounding box (x_center, y_center, width, height) to \n",
        "    segmentation polygon format (x1 y1 x2 y2 x3 y3 x4 y4).\n",
        "    \n",
        "    This is required for training segmentation models with detection datasets.\n",
        "    Returns None if the input is invalid or results in out-of-bounds coordinates.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        xc, yc, w, h = map(float, bbox_coords)\n",
        "        # Validate input values are in valid range [0, 1]\n",
        "        if not (0 <= xc <= 1 and 0 <= yc <= 1 and 0 < w <= 1 and 0 < h <= 1):\n",
        "            return None\n",
        "        # Calculate corner points (normalized coordinates)\n",
        "        x1, y1 = xc - w/2, yc - h/2  # Top-left\n",
        "        x2, y2 = xc + w/2, yc - h/2  # Top-right\n",
        "        x3, y3 = xc + w/2, yc + h/2  # Bottom-right\n",
        "        x4, y4 = xc - w/2, yc + h/2  # Bottom-left\n",
        "        # Clamp values to [0, 1] range\n",
        "        x1, y1 = max(0, x1), max(0, y1)\n",
        "        x2, y2 = min(1, x2), max(0, y2)\n",
        "        x3, y3 = min(1, x3), min(1, y3)\n",
        "        x4, y4 = max(0, x4), min(1, y4)\n",
        "        return f\"{x1} {y1} {x2} {y2} {x3} {y3} {x4} {y4}\"\n",
        "    except (ValueError, TypeError):\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ Configuration loaded!\")\n",
        "print(f\"Target classes: {TARGET_CLASSES}\")\n",
        "print(f\"\\nüìå Note: Labels will be converted to segmentation format (polygon coordinates)\")"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download datasets\n",
        "!mkdir -p /content/raw_datasets\n",
        "%cd /content/raw_datasets\n",
        "\n",
        "for name, url in DATASETS:\n",
        "    if not os.path.exists(name):\n",
        "        print(f\"üì• Downloading {name}...\")\n",
        "        !mkdir -p {name}\n",
        "        !curl -L \"{url}\" > {name}/dataset.zip 2>/dev/null\n",
        "        !unzip -q {name}/dataset.zip -d {name}\n",
        "        !rm {name}/dataset.zip\n",
        "    else:\n",
        "        print(f\"‚úì {name} already exists\")\n",
        "\n",
        "print(\"\\n‚úÖ All datasets downloaded!\")"
      ],
      "metadata": {
        "id": "download_datasets"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge and convert datasets to SEGMENTATION format\n",
        "OUTPUT_DIR = '/content/merged_dataset'\n",
        "\n",
        "# Clear and create output directory\n",
        "!rm -rf {OUTPUT_DIR}\n",
        "os.makedirs(f\"{OUTPUT_DIR}/train/images\", exist_ok=True)\n",
        "os.makedirs(f\"{OUTPUT_DIR}/train/labels\", exist_ok=True)\n",
        "os.makedirs(f\"{OUTPUT_DIR}/valid/images\", exist_ok=True)\n",
        "os.makedirs(f\"{OUTPUT_DIR}/valid/labels\", exist_ok=True)\n",
        "\n",
        "total_train = 0\n",
        "total_valid = 0\n",
        "bbox_converted = 0  # Count of bounding boxes converted to segments\n",
        "seg_preserved = 0   # Count of segmentation labels preserved\n",
        "\n",
        "for name, _ in DATASETS:\n",
        "    dataset_dir = f\"/content/raw_datasets/{name}\"\n",
        "    if not os.path.exists(dataset_dir):\n",
        "        continue\n",
        "    \n",
        "    # Find data.yaml\n",
        "    data_yaml = None\n",
        "    for root, dirs, files in os.walk(dataset_dir):\n",
        "        if 'data.yaml' in files:\n",
        "            data_yaml = os.path.join(root, 'data.yaml')\n",
        "            break\n",
        "    \n",
        "    if not data_yaml:\n",
        "        print(f\"‚ö†Ô∏è No data.yaml in {name}\")\n",
        "        continue\n",
        "    \n",
        "    with open(data_yaml, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    \n",
        "    source_classes = config.get('names', [])\n",
        "    if isinstance(source_classes, dict):\n",
        "        source_classes = list(source_classes.values())\n",
        "    \n",
        "    print(f\"\\nüìÇ Processing {name}...\")\n",
        "    print(f\"   Source classes: {source_classes}\")\n",
        "    \n",
        "    for split in ['train', 'valid', 'test']:\n",
        "        img_dir = None\n",
        "        lbl_dir = None\n",
        "        \n",
        "        # Try different directory structures\n",
        "        for try_path in [dataset_dir, os.path.dirname(data_yaml)]:\n",
        "            if os.path.exists(os.path.join(try_path, split, 'images')):\n",
        "                img_dir = os.path.join(try_path, split, 'images')\n",
        "                lbl_dir = os.path.join(try_path, split, 'labels')\n",
        "                break\n",
        "        \n",
        "        if not img_dir or not os.path.exists(img_dir):\n",
        "            continue\n",
        "        \n",
        "        out_split = 'train' if split in ['train', 'test'] else 'valid'\n",
        "        count = 0\n",
        "        \n",
        "        for img_file in os.listdir(img_dir):\n",
        "            if not img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                continue\n",
        "            \n",
        "            # Copy image\n",
        "            src_img = os.path.join(img_dir, img_file)\n",
        "            dst_img = os.path.join(OUTPUT_DIR, out_split, 'images', f\"{name}_{img_file}\")\n",
        "            shutil.copy(src_img, dst_img)\n",
        "            \n",
        "            # Convert label to SEGMENTATION format\n",
        "            lbl_file = os.path.splitext(img_file)[0] + '.txt'\n",
        "            src_lbl = os.path.join(lbl_dir, lbl_file)\n",
        "            dst_lbl = os.path.join(OUTPUT_DIR, out_split, 'labels', f\"{name}_{lbl_file}\")\n",
        "            \n",
        "            if os.path.exists(src_lbl):\n",
        "                with open(src_lbl, 'r') as f:\n",
        "                    lines = f.readlines()\n",
        "                \n",
        "                new_lines = []\n",
        "                for line in lines:\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) < 5:\n",
        "                        continue\n",
        "                    \n",
        "                    old_class_id = int(parts[0])\n",
        "                    if old_class_id < len(source_classes):\n",
        "                        old_class_name = source_classes[old_class_id]\n",
        "                        new_class_id = normalize_class(old_class_name)\n",
        "                        \n",
        "                        if new_class_id >= 0:\n",
        "                            # Check if this is bounding box (5 parts) or segmentation (9+ parts)\n",
        "                            # Bounding box: class x_center y_center width height (5 values)\n",
        "                            # Segmentation: class x1 y1 x2 y2 x3 y3 x4 y4... (9+ values for rectangle/polygon)\n",
        "                            if len(parts) == 5:\n",
        "                                # Bounding box format: class x_center y_center width height\n",
        "                                # Convert to segmentation polygon format\n",
        "                                segment_coords = bbox_to_segment(parts[1:])\n",
        "                                if segment_coords:\n",
        "                                    new_lines.append(f\"{new_class_id} {segment_coords}\")\n",
        "                                    bbox_converted += 1\n",
        "                            elif len(parts) >= 9:\n",
        "                                # Already segmentation format: class x1 y1 x2 y2 x3 y3 x4 y4 ...\n",
        "                                new_lines.append(f\"{new_class_id} {' '.join(parts[1:])}\")\n",
        "                                seg_preserved += 1\n",
        "                            # Skip labels with 6-8 parts as they're likely malformed\n",
        "                \n",
        "                if new_lines:\n",
        "                    with open(dst_lbl, 'w') as f:\n",
        "                        f.write('\\n'.join(new_lines))\n",
        "                    count += 1\n",
        "        \n",
        "        if out_split == 'train':\n",
        "            total_train += count\n",
        "        else:\n",
        "            total_valid += count\n",
        "        \n",
        "        if count > 0:\n",
        "            print(f\"   {split} -> {out_split}: {count} images\")\n",
        "\n",
        "# Create data.yaml\n",
        "data_yaml_content = {\n",
        "    'path': OUTPUT_DIR,\n",
        "    'train': 'train/images',\n",
        "    'val': 'valid/images',\n",
        "    'names': {i: name for i, name in enumerate(TARGET_CLASSES)},\n",
        "    'nc': len(TARGET_CLASSES),\n",
        "}\n",
        "\n",
        "with open(f\"{OUTPUT_DIR}/data.yaml\", 'w') as f:\n",
        "    yaml.dump(data_yaml_content, f, default_flow_style=False)\n",
        "\n",
        "print(f\"\\n\" + \"=\"*50)\n",
        "print(f\"‚úÖ Dataset prepared for SEGMENTATION training!\")\n",
        "print(f\"   Train images: {total_train}\")\n",
        "print(f\"   Valid images: {total_valid}\")\n",
        "print(f\"   Classes: {TARGET_CLASSES}\")\n",
        "print(f\"\\nüìä Label conversion:\")\n",
        "print(f\"   Bounding boxes converted to polygons: {bbox_converted}\")\n",
        "print(f\"   Segmentation labels preserved: {seg_preserved}\")\n",
        "print(f\"=\"*50)"
      ],
      "metadata": {
        "id": "merge_datasets"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload or specify base model path\n",
        "# IMPORTANT: Use a SEGMENTATION model (yolo11s-seg.pt, yolo11x-seg.pt, etc.)\n",
        "\n",
        "# Option 1: Upload from local machine\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()  # Upload best.pt\n",
        "# BASE_MODEL_PATH = list(uploaded.keys())[0]\n",
        "\n",
        "# Option 2: Use from Google Drive (recommended) - your existing segmentation model\n",
        "BASE_MODEL_PATH = '/content/drive/MyDrive/Intelligence-Test-Models/anticheat_objects_v2_headphones/weights/best.pt'\n",
        "\n",
        "# Option 3: Start from pretrained YOLO11 SEGMENTATION model\n",
        "# BASE_MODEL_PATH = 'yolo11s-seg.pt'  # Note: use -seg variant!\n",
        "\n",
        "print(f\"Base model: {BASE_MODEL_PATH}\")\n",
        "if os.path.exists(BASE_MODEL_PATH):\n",
        "    print(\"‚úÖ Model file found!\")\n",
        "    # Verify it's a segmentation model\n",
        "    if '-seg' in BASE_MODEL_PATH.lower() or 'seg' in BASE_MODEL_PATH.lower():\n",
        "        print(\"‚úÖ Confirmed: This appears to be a segmentation model\")\n",
        "else:\n",
        "    if BASE_MODEL_PATH.startswith('yolo'):\n",
        "        print(f\"üì• Will download pretrained model: {BASE_MODEL_PATH}\")\n",
        "    else:\n",
        "        print(\"‚ùå Model file not found! Please check path.\")\n",
        "        print(\"\\nüí° TIP: For segmentation, use models like:\")\n",
        "        print(\"   - yolo11s-seg.pt (small, fast)\")\n",
        "        print(\"   - yolo11m-seg.pt (medium)\")\n",
        "        print(\"   - yolo11x-seg.pt (large, accurate)\")"
      ],
      "metadata": {
        "id": "setup_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune the SEGMENTATION model\n",
        "model = YOLO(BASE_MODEL_PATH)\n",
        "\n",
        "print(\"üöÄ Starting fine-tuning SEGMENTATION model...\")\n",
        "print(\"This may take 30-60 minutes depending on dataset size.\")\n",
        "\n",
        "results = model.train(\n",
        "    data=f\"{OUTPUT_DIR}/data.yaml\",\n",
        "    epochs=50,              # Number of epochs\n",
        "    imgsz=640,              # Image size\n",
        "    batch=16,               # Batch size (reduce if OOM)\n",
        "    patience=15,            # Early stopping\n",
        "    lr0=0.001,              # Lower LR for fine-tuning\n",
        "    lrf=0.01,               # Final LR factor\n",
        "    warmup_epochs=3,        # Warmup\n",
        "    freeze=10,              # Freeze first 10 layers\n",
        "    project='/content/drive/MyDrive/Intelligence-Test-Models',\n",
        "    name='anticheat_finetuned_seg_v3',\n",
        "    exist_ok=True,\n",
        "    device=0,               # GPU\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Training completed!\")"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export to ONNX\n",
        "BEST_MODEL_PATH = '/content/drive/MyDrive/Intelligence-Test-Models/anticheat_finetuned_seg_v3/weights/best.pt'\n",
        "\n",
        "model = YOLO(BEST_MODEL_PATH)\n",
        "\n",
        "print(\"üì¶ Exporting segmentation model to ONNX...\")\n",
        "model.export(\n",
        "    format='onnx',\n",
        "    imgsz=640,\n",
        "    simplify=True,\n",
        "    dynamic=False,\n",
        "    opset=17\n",
        ")\n",
        "\n",
        "onnx_path = BEST_MODEL_PATH.replace('.pt', '.onnx')\n",
        "print(f\"\\n‚úÖ ONNX model saved to: {onnx_path}\")"
      ],
      "metadata": {
        "id": "export"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the new model\n",
        "import onnxruntime as ort\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "session = ort.InferenceSession(onnx_path)\n",
        "input_name = session.get_inputs()[0].name\n",
        "classes = ['person', 'phone', 'material', 'headphones']\n",
        "\n",
        "def test_image(name, img):\n",
        "    img = img.convert('RGB').resize((640, 640))\n",
        "    img_array = np.array(img).astype(np.float32) / 255.0\n",
        "    img_array = np.transpose(img_array, (2, 0, 1))\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    \n",
        "    outputs = session.run(None, {input_name: img_array})\n",
        "    # Segmentation models output: (detection_output, mask_output)\n",
        "    output = outputs[0]\n",
        "    class_scores = output[0, 4:8, :]\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    for i, cls in enumerate(classes):\n",
        "        max_score = class_scores[i].max()\n",
        "        print(f\"  {cls}: max={max_score:.4f}\")\n",
        "\n",
        "# Test with phone simulation\n",
        "phone_img = Image.new('RGB', (640, 640), color=(200, 200, 200))\n",
        "draw = ImageDraw.Draw(phone_img)\n",
        "draw.rectangle([250, 200, 390, 450], fill=(20, 20, 20))\n",
        "draw.rectangle([260, 210, 380, 440], fill=(50, 50, 80))\n",
        "test_image(\"Phone simulation\", phone_img)\n",
        "\n",
        "# Test with headphones simulation\n",
        "headphones_img = Image.new('RGB', (640, 640), color=(200, 200, 200))\n",
        "draw = ImageDraw.Draw(headphones_img)\n",
        "draw.arc([200, 150, 440, 350], 180, 0, fill=(30, 30, 30), width=20)\n",
        "draw.ellipse([180, 280, 260, 400], fill=(40, 40, 40))\n",
        "draw.ellipse([380, 280, 460, 400], fill=(40, 40, 40))\n",
        "test_image(\"Headphones simulation\", headphones_img)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"If phone scores improved (>0.1), the fine-tuning worked!\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "test"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the ONNX file\n",
        "print(\"\\nüìã NEXT STEPS:\")\n",
        "print(\"1. Download the ONNX file from Google Drive\")\n",
        "print(f\"   Location: {onnx_path}\")\n",
        "print(\"2. Rename it to: anticheat_yolo11s.onnx\")\n",
        "print(\"3. Copy to: Intelligence-Test/public/models/\")\n",
        "print(\"4. Rebuild and deploy the web app\")\n",
        "print(\"\\nüéâ Done!\")"
      ],
      "metadata": {
        "id": "finish"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
