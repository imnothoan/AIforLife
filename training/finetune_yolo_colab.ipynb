{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# ğŸ¯ YOLO Segmentation Anti-Cheat Model Fine-tuning\n","\n","**IMPORTANT**: This notebook is for training **YOLO Segmentation models** (yolo11s-seg, yolo11x-seg, etc.).\n","\n","This notebook will fine-tune the existing YOLO segmentation model to improve detection of:\n","- ğŸ“± **Phone** (currently weak - max ~0.2%)\n","- ğŸ“„ **Material/Paper** (currently weak - max ~0.1%)\n","- ğŸ‘¤ **Person** (needs improvement - max ~5%)\n","- ğŸ§ **Headphones** (already good - max ~44%)\n","\n","## Instructions:\n","1. Upload your existing `best.pt` segmentation model to Google Drive\n","2. Run all cells in order\n","3. Download the new `best.onnx` file when done\n","4. Replace `Intelligence-Test/public/models/anticheat_yolo11s.onnx`\n","\n","## Note on Dataset Format:\n","This notebook automatically converts bounding box labels to segmentation polygon format\n","so that detection datasets can be used to train segmentation models."],"metadata":{"id":"header"}},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"mount_drive","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765792191399,"user_tz":-420,"elapsed":68085,"user":{"displayName":"Pháº¡m Ngá»c Huy HoÃ n","userId":"05738996591361131973"}},"outputId":"0d2feae4-01d9-4159-a38d-38d97dab8f74"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Install dependencies\n","!pip install ultralytics -q\n","!pip install onnx onnxruntime -q\n","\n","import os\n","import shutil\n","import yaml\n","import numpy as np\n","from pathlib import Path\n","from ultralytics import YOLO\n","\n","print(\"âœ… Dependencies installed!\")"],"metadata":{"id":"install_deps","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765792221591,"user_tz":-420,"elapsed":27723,"user":{"displayName":"Pháº¡m Ngá»c Huy HoÃ n","userId":"05738996591361131973"}},"outputId":"0c92ed28-53ba-44ad-be88-e8dcac83f728"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCreating new Ultralytics Settings v0.0.6 file âœ… \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","âœ… Dependencies installed!\n"]}]},{"cell_type":"code","source":["# Configuration\n","TARGET_CLASSES = ['person', 'phone', 'material', 'headphones']\n","\n","# Dataset URLs from Roboflow (phone and material focused)\n","DATASETS = [\n","    # Phone datasets (HIGH PRIORITY - model is weak here)\n","    (\"phone_1\", \"https://app.roboflow.com/ds/5ReObgnLbQ?key=HTPSgVzDLW\"),\n","    (\"phone_2\", \"https://app.roboflow.com/ds/f9k54F7Azq?key=eYssUekSYc\"),\n","\n","    # Paper/Material datasets (HIGH PRIORITY)\n","    (\"paper_1\", \"https://app.roboflow.com/ds/inuabMtp6t?key=jbu7HTlrBf\"),\n","    (\"paper_2\", \"https://app.roboflow.com/ds/b4oxAhlW40?key=4A761Kjm5F\"),\n","\n","    # Headphones (for balance)\n","    (\"headphones_1\", \"https://app.roboflow.com/ds/qqqEeSKAlk?key=GT1Xa65onI\"),\n","    (\"headphones_2\", \"https://app.roboflow.com/ds/cKHwOqmuda?key=qL10KsWlBt\"),\n","\n","    # Person\n","    (\"person_1\", \"https://app.roboflow.com/ds/PwRwV0c1jL?key=FgXbXeqlpH\"),\n","]\n","\n","# Class mapping\n","CLASS_MAPPING = {\n","    'person': 'person', 'student': 'person', 'face': 'person', 'head': 'person',\n","    'human': 'person', 'people': 'person', 'man': 'person', 'woman': 'person',\n","    'phone': 'phone', 'mobile': 'phone', 'cell phone': 'phone',\n","    'telephone': 'phone', 'smartphone': 'phone', 'cellphone': 'phone',\n","    'mobile phone': 'phone', 'iphone': 'phone', 'android': 'phone',\n","    'ProductRecog - v2 2024-11-05 7:03am': 'phone',\n","    'paper': 'material', 'document': 'material', 'book': 'material',\n","    'notebook': 'material', 'notes': 'material', 'sheet': 'material',\n","    'material': 'material', 'cheat sheet': 'material', 'PAPER': 'material', 'Paper': 'material',\n","    'headphone': 'headphones', 'headphones': 'headphones',\n","    'earphone': 'headphones', 'earphones': 'headphones',\n","    'headset': 'headphones', 'earbuds': 'headphones', 'earbud': 'headphones',\n","    'airpods': 'headphones', 'ear device': 'headphones', 'Headphone': 'headphones',\n","    'left earbud': 'headphones', 'eardevice': 'headphones',\n","}\n","\n","def normalize_class(class_name):\n","    class_name = class_name.lower().strip()\n","    for key, target in CLASS_MAPPING.items():\n","        if key.lower() == class_name:\n","            return TARGET_CLASSES.index(target)\n","    return -1\n","\n","def bbox_to_segment(bbox_coords):\n","    \"\"\"\n","    Convert YOLO bounding box (x_center, y_center, width, height) to\n","    segmentation polygon format (x1 y1 x2 y2 x3 y3 x4 y4).\n","\n","    This is required for training segmentation models with detection datasets.\n","    Returns None if the input is invalid or results in out-of-bounds coordinates.\n","    \"\"\"\n","    try:\n","        xc, yc, w, h = map(float, bbox_coords)\n","        # Validate input values are in valid range [0, 1]\n","        if not (0 <= xc <= 1 and 0 <= yc <= 1 and 0 < w <= 1 and 0 < h <= 1):\n","            return None\n","        # Calculate corner points (normalized coordinates)\n","        x1, y1 = xc - w/2, yc - h/2  # Top-left\n","        x2, y2 = xc + w/2, yc - h/2  # Top-right\n","        x3, y3 = xc + w/2, yc + h/2  # Bottom-right\n","        x4, y4 = xc - w/2, yc + h/2  # Bottom-left\n","        # Clamp values to [0, 1] range\n","        x1, y1 = max(0, x1), max(0, y1)\n","        x2, y2 = min(1, x2), max(0, y2)\n","        x3, y3 = min(1, x3), min(1, y3)\n","        x4, y4 = max(0, x4), min(1, y4)\n","        return f\"{x1} {y1} {x2} {y2} {x3} {y3} {x4} {y4}\"\n","    except (ValueError, TypeError):\n","        return None\n","\n","print(\"âœ… Configuration loaded!\")\n","print(f\"Target classes: {TARGET_CLASSES}\")\n","print(f\"\\nğŸ“Œ Note: Labels will be converted to segmentation format (polygon coordinates)\")"],"metadata":{"id":"config","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765792224147,"user_tz":-420,"elapsed":310,"user":{"displayName":"Pháº¡m Ngá»c Huy HoÃ n","userId":"05738996591361131973"}},"outputId":"e23649a8-456f-42c5-afd9-797268ea0432"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Configuration loaded!\n","Target classes: ['person', 'phone', 'material', 'headphones']\n","\n","ğŸ“Œ Note: Labels will be converted to segmentation format (polygon coordinates)\n"]}]},{"cell_type":"code","source":["# Download datasets\n","!mkdir -p /content/raw_datasets\n","%cd /content/raw_datasets\n","\n","for name, url in DATASETS:\n","    if not os.path.exists(name):\n","        print(f\"ğŸ“¥ Downloading {name}...\")\n","        !mkdir -p {name}\n","        !curl -L \"{url}\" > {name}/dataset.zip 2>/dev/null\n","        !unzip -q {name}/dataset.zip -d {name}\n","        !rm {name}/dataset.zip\n","    else:\n","        print(f\"âœ“ {name} already exists\")\n","\n","print(\"\\nâœ… All datasets downloaded!\")"],"metadata":{"id":"download_datasets","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765792272580,"user_tz":-420,"elapsed":32628,"user":{"displayName":"Pháº¡m Ngá»c Huy HoÃ n","userId":"05738996591361131973"}},"outputId":"30a6b7c7-421d-4a2b-c4f2-a2976b09f454"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/raw_datasets\n","ğŸ“¥ Downloading phone_1...\n","ğŸ“¥ Downloading phone_2...\n","ğŸ“¥ Downloading paper_1...\n","ğŸ“¥ Downloading paper_2...\n","ğŸ“¥ Downloading headphones_1...\n","ğŸ“¥ Downloading headphones_2...\n","ğŸ“¥ Downloading person_1...\n","\n","âœ… All datasets downloaded!\n"]}]},{"cell_type":"code","source":["# Merge and convert datasets to SEGMENTATION format\n","OUTPUT_DIR = '/content/merged_dataset'\n","\n","# Clear and create output directory\n","!rm -rf {OUTPUT_DIR}\n","os.makedirs(f\"{OUTPUT_DIR}/train/images\", exist_ok=True)\n","os.makedirs(f\"{OUTPUT_DIR}/train/labels\", exist_ok=True)\n","os.makedirs(f\"{OUTPUT_DIR}/valid/images\", exist_ok=True)\n","os.makedirs(f\"{OUTPUT_DIR}/valid/labels\", exist_ok=True)\n","\n","total_train = 0\n","total_valid = 0\n","bbox_converted = 0  # Count of bounding boxes converted to segments\n","seg_preserved = 0   # Count of segmentation labels preserved\n","\n","for name, _ in DATASETS:\n","    dataset_dir = f\"/content/raw_datasets/{name}\"\n","    if not os.path.exists(dataset_dir):\n","        continue\n","\n","    # Find data.yaml\n","    data_yaml = None\n","    for root, dirs, files in os.walk(dataset_dir):\n","        if 'data.yaml' in files:\n","            data_yaml = os.path.join(root, 'data.yaml')\n","            break\n","\n","    if not data_yaml:\n","        print(f\"âš ï¸ No data.yaml in {name}\")\n","        continue\n","\n","    with open(data_yaml, 'r') as f:\n","        config = yaml.safe_load(f)\n","\n","    source_classes = config.get('names', [])\n","    if isinstance(source_classes, dict):\n","        source_classes = list(source_classes.values())\n","\n","    print(f\"\\nğŸ“‚ Processing {name}...\")\n","    print(f\"   Source classes: {source_classes}\")\n","\n","    for split in ['train', 'valid', 'test']:\n","        img_dir = None\n","        lbl_dir = None\n","\n","        # Try different directory structures\n","        for try_path in [dataset_dir, os.path.dirname(data_yaml)]:\n","            if os.path.exists(os.path.join(try_path, split, 'images')):\n","                img_dir = os.path.join(try_path, split, 'images')\n","                lbl_dir = os.path.join(try_path, split, 'labels')\n","                break\n","\n","        if not img_dir or not os.path.exists(img_dir):\n","            continue\n","\n","        out_split = 'train' if split in ['train', 'test'] else 'valid'\n","        count = 0\n","\n","        for img_file in os.listdir(img_dir):\n","            if not img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n","                continue\n","\n","            # Copy image\n","            src_img = os.path.join(img_dir, img_file)\n","            dst_img = os.path.join(OUTPUT_DIR, out_split, 'images', f\"{name}_{img_file}\")\n","            shutil.copy(src_img, dst_img)\n","\n","            # Convert label to SEGMENTATION format\n","            lbl_file = os.path.splitext(img_file)[0] + '.txt'\n","            src_lbl = os.path.join(lbl_dir, lbl_file)\n","            dst_lbl = os.path.join(OUTPUT_DIR, out_split, 'labels', f\"{name}_{lbl_file}\")\n","\n","            if os.path.exists(src_lbl):\n","                with open(src_lbl, 'r') as f:\n","                    lines = f.readlines()\n","\n","                new_lines = []\n","                for line in lines:\n","                    parts = line.strip().split()\n","                    if len(parts) < 5:\n","                        continue\n","\n","                    old_class_id = int(parts[0])\n","                    if old_class_id < len(source_classes):\n","                        old_class_name = source_classes[old_class_id]\n","                        new_class_id = normalize_class(old_class_name)\n","\n","                        if new_class_id >= 0:\n","                            # Check if this is bounding box (5 parts) or segmentation (9+ parts)\n","                            # Bounding box: class x_center y_center width height (5 values)\n","                            # Segmentation: class x1 y1 x2 y2 x3 y3 x4 y4... (9+ values for rectangle/polygon)\n","                            if len(parts) == 5:\n","                                # Bounding box format: class x_center y_center width height\n","                                # Convert to segmentation polygon format\n","                                segment_coords = bbox_to_segment(parts[1:])\n","                                if segment_coords:\n","                                    new_lines.append(f\"{new_class_id} {segment_coords}\")\n","                                    bbox_converted += 1\n","                            elif len(parts) >= 9:\n","                                # Already segmentation format: class x1 y1 x2 y2 x3 y3 x4 y4 ...\n","                                new_lines.append(f\"{new_class_id} {' '.join(parts[1:])}\")\n","                                seg_preserved += 1\n","                            # Skip labels with 6-8 parts as they're likely malformed\n","\n","                if new_lines:\n","                    with open(dst_lbl, 'w') as f:\n","                        f.write('\\n'.join(new_lines))\n","                    count += 1\n","\n","        if out_split == 'train':\n","            total_train += count\n","        else:\n","            total_valid += count\n","\n","        if count > 0:\n","            print(f\"   {split} -> {out_split}: {count} images\")\n","\n","# Create data.yaml\n","data_yaml_content = {\n","    'path': OUTPUT_DIR,\n","    'train': 'train/images',\n","    'val': 'valid/images',\n","    'names': {i: name for i, name in enumerate(TARGET_CLASSES)},\n","    'nc': len(TARGET_CLASSES),\n","}\n","\n","with open(f\"{OUTPUT_DIR}/data.yaml\", 'w') as f:\n","    yaml.dump(data_yaml_content, f, default_flow_style=False)\n","\n","print(f\"\\n\" + \"=\"*50)\n","print(f\"âœ… Dataset prepared for SEGMENTATION training!\")\n","print(f\"   Train images: {total_train}\")\n","print(f\"   Valid images: {total_valid}\")\n","print(f\"   Classes: {TARGET_CLASSES}\")\n","print(f\"\\nğŸ“Š Label conversion:\")\n","print(f\"   Bounding boxes converted to polygons: {bbox_converted}\")\n","print(f\"   Segmentation labels preserved: {seg_preserved}\")\n","print(f\"=\"*50)"],"metadata":{"id":"merge_datasets","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765792282327,"user_tz":-420,"elapsed":5951,"user":{"displayName":"Pháº¡m Ngá»c Huy HoÃ n","userId":"05738996591361131973"}},"outputId":"ab6492ea-9ddb-4516-815c-a57f777f1191"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸ“‚ Processing phone_1...\n","   Source classes: ['ProductRecog - v2 2024-11-05 7-03am', 'phone']\n","   train -> train: 103 images\n","   valid -> valid: 29 images\n","   test -> train: 15 images\n","\n","ğŸ“‚ Processing phone_2...\n","   Source classes: ['6', 'phone', 'undefined']\n","   train -> train: 1216 images\n","   valid -> valid: 266 images\n","   test -> train: 181 images\n","\n","ğŸ“‚ Processing paper_1...\n","   Source classes: ['document']\n","   train -> train: 3459 images\n","   valid -> valid: 453 images\n","   test -> train: 1174 images\n","\n","ğŸ“‚ Processing paper_2...\n","   Source classes: ['paper']\n","   train -> train: 52 images\n","   valid -> valid: 16 images\n","   test -> train: 7 images\n","\n","ğŸ“‚ Processing headphones_1...\n","   Source classes: ['earphone']\n","   train -> train: 82 images\n","   valid -> valid: 14 images\n","   test -> train: 8 images\n","\n","ğŸ“‚ Processing headphones_2...\n","   Source classes: ['earphone']\n","   train -> train: 362 images\n","   valid -> valid: 105 images\n","   test -> train: 44 images\n","\n","ğŸ“‚ Processing person_1...\n","   Source classes: ['book']\n","   train -> train: 100 images\n","\n","==================================================\n","âœ… Dataset prepared for SEGMENTATION training!\n","   Train images: 6803\n","   Valid images: 883\n","   Classes: ['person', 'phone', 'material', 'headphones']\n","\n","ğŸ“Š Label conversion:\n","   Bounding boxes converted to polygons: 3055\n","   Segmentation labels preserved: 5281\n","==================================================\n"]}]},{"cell_type":"code","source":["# Upload or specify base model path\n","# IMPORTANT: Use a SEGMENTATION model (yolo11s-seg.pt, yolo11x-seg.pt, etc.)\n","\n","# Option 1: Upload from local machine\n","# from google.colab import files\n","# uploaded = files.upload()  # Upload best.pt\n","# BASE_MODEL_PATH = list(uploaded.keys())[0]\n","\n","# Option 2: Use from Google Drive (recommended) - your existing segmentation model\n","BASE_MODEL_PATH = '/content/drive/MyDrive/Intelligence-Test-Models/anticheat_objects_v2_headphones/weights/best.pt'\n","\n","# Option 3: Start from pretrained YOLO11 SEGMENTATION model\n","# BASE_MODEL_PATH = 'yolo11s-seg.pt'  # Note: use -seg variant!\n","\n","print(f\"Base model: {BASE_MODEL_PATH}\")\n","if os.path.exists(BASE_MODEL_PATH):\n","    print(\"âœ… Model file found!\")\n","    # Verify it's a segmentation model\n","    if '-seg' in BASE_MODEL_PATH.lower() or 'seg' in BASE_MODEL_PATH.lower():\n","        print(\"âœ… Confirmed: This appears to be a segmentation model\")\n","else:\n","    if BASE_MODEL_PATH.startswith('yolo'):\n","        print(f\"ğŸ“¥ Will download pretrained model: {BASE_MODEL_PATH}\")\n","    else:\n","        print(\"âŒ Model file not found! Please check path.\")\n","        print(\"\\nğŸ’¡ TIP: For segmentation, use models like:\")\n","        print(\"   - yolo11s-seg.pt (small, fast)\")\n","        print(\"   - yolo11m-seg.pt (medium)\")\n","        print(\"   - yolo11x-seg.pt (large, accurate)\")"],"metadata":{"id":"setup_model","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765792292737,"user_tz":-420,"elapsed":1108,"user":{"displayName":"Pháº¡m Ngá»c Huy HoÃ n","userId":"05738996591361131973"}},"outputId":"d4ef41bb-d5e1-4d4a-e15a-2f6232fec572"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Base model: /content/drive/MyDrive/Intelligence-Test-Models/anticheat_objects_v2_headphones/weights/best.pt\n","âœ… Model file found!\n"]}]},{"cell_type":"code","source":["# Fine-tune the SEGMENTATION model\n","model = YOLO(BASE_MODEL_PATH)\n","\n","print(\"ğŸš€ Starting fine-tuning SEGMENTATION model...\")\n","print(\"This may take 30-60 minutes depending on dataset size.\")\n","\n","results = model.train(\n","    data=f\"{OUTPUT_DIR}/data.yaml\",\n","    epochs=50,              # Number of epochs\n","    imgsz=640,              # Image size\n","    batch=16,               # Batch size (reduce if OOM)\n","    patience=15,            # Early stopping\n","    lr0=0.001,              # Lower LR for fine-tuning\n","    lrf=0.01,               # Final LR factor\n","    warmup_epochs=3,        # Warmup\n","    freeze=10,              # Freeze first 10 layers\n","    project='/content/drive/MyDrive/Intelligence-Test-Models',\n","    name='anticheat_finetuned_seg_v3',\n","    exist_ok=True,\n","    device=0,               # GPU\n","    verbose=True,\n",")\n","\n","print(\"\\nâœ… Training completed!\")"],"metadata":{"id":"train","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765802535652,"user_tz":-420,"elapsed":10235422,"user":{"displayName":"Pháº¡m Ngá»c Huy HoÃ n","userId":"05738996591361131973"}},"outputId":"4e9fff35-ffea-4d8e-d77c-731090773278"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸš€ Starting fine-tuning SEGMENTATION model...\n","This may take 30-60 minutes depending on dataset size.\n","Ultralytics 8.3.237 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/merged_dataset/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=10, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=/content/drive/MyDrive/Intelligence-Test-Models/anticheat_objects_v2_headphones/weights/best.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=anticheat_finetuned_seg_v3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=15, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/Intelligence-Test-Models, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/Intelligence-Test-Models/anticheat_finetuned_seg_v3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n","\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 32.2MB/s 0.0s\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n","  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n","  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n","  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n","  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n","  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n","  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n","  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n","  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n","  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n"," 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n"," 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 13                  -1  1    443776  ultralytics.nn.modules.block.C3k2            [768, 256, 1, False]          \n"," 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 16                  -1  1    127680  ultralytics.nn.modules.block.C3k2            [512, 128, 1, False]          \n"," 17                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n"," 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 19                  -1  1    345472  ultralytics.nn.modules.block.C3k2            [384, 256, 1, False]          \n"," 20                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n"," 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 22                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n"," 23        [16, 19, 22]  1   1475452  ultralytics.nn.modules.head.Segment          [4, 32, 128, [128, 256, 512]] \n","YOLO11s-seg summary: 203 layers, 10,083,836 parameters, 10,083,820 gradients, 33.1 GFLOPs\n","\n","Transferred 561/561 items from pretrained weights\n","Freezing layer 'model.0.conv.weight'\n","Freezing layer 'model.0.bn.weight'\n","Freezing layer 'model.0.bn.bias'\n","Freezing layer 'model.1.conv.weight'\n","Freezing layer 'model.1.bn.weight'\n","Freezing layer 'model.1.bn.bias'\n","Freezing layer 'model.2.cv1.conv.weight'\n","Freezing layer 'model.2.cv1.bn.weight'\n","Freezing layer 'model.2.cv1.bn.bias'\n","Freezing layer 'model.2.cv2.conv.weight'\n","Freezing layer 'model.2.cv2.bn.weight'\n","Freezing layer 'model.2.cv2.bn.bias'\n","Freezing layer 'model.2.m.0.cv1.conv.weight'\n","Freezing layer 'model.2.m.0.cv1.bn.weight'\n","Freezing layer 'model.2.m.0.cv1.bn.bias'\n","Freezing layer 'model.2.m.0.cv2.conv.weight'\n","Freezing layer 'model.2.m.0.cv2.bn.weight'\n","Freezing layer 'model.2.m.0.cv2.bn.bias'\n","Freezing layer 'model.3.conv.weight'\n","Freezing layer 'model.3.bn.weight'\n","Freezing layer 'model.3.bn.bias'\n","Freezing layer 'model.4.cv1.conv.weight'\n","Freezing layer 'model.4.cv1.bn.weight'\n","Freezing layer 'model.4.cv1.bn.bias'\n","Freezing layer 'model.4.cv2.conv.weight'\n","Freezing layer 'model.4.cv2.bn.weight'\n","Freezing layer 'model.4.cv2.bn.bias'\n","Freezing layer 'model.4.m.0.cv1.conv.weight'\n","Freezing layer 'model.4.m.0.cv1.bn.weight'\n","Freezing layer 'model.4.m.0.cv1.bn.bias'\n","Freezing layer 'model.4.m.0.cv2.conv.weight'\n","Freezing layer 'model.4.m.0.cv2.bn.weight'\n","Freezing layer 'model.4.m.0.cv2.bn.bias'\n","Freezing layer 'model.5.conv.weight'\n","Freezing layer 'model.5.bn.weight'\n","Freezing layer 'model.5.bn.bias'\n","Freezing layer 'model.6.cv1.conv.weight'\n","Freezing layer 'model.6.cv1.bn.weight'\n","Freezing layer 'model.6.cv1.bn.bias'\n","Freezing layer 'model.6.cv2.conv.weight'\n","Freezing layer 'model.6.cv2.bn.weight'\n","Freezing layer 'model.6.cv2.bn.bias'\n","Freezing layer 'model.6.m.0.cv1.conv.weight'\n","Freezing layer 'model.6.m.0.cv1.bn.weight'\n","Freezing layer 'model.6.m.0.cv1.bn.bias'\n","Freezing layer 'model.6.m.0.cv2.conv.weight'\n","Freezing layer 'model.6.m.0.cv2.bn.weight'\n","Freezing layer 'model.6.m.0.cv2.bn.bias'\n","Freezing layer 'model.6.m.0.cv3.conv.weight'\n","Freezing layer 'model.6.m.0.cv3.bn.weight'\n","Freezing layer 'model.6.m.0.cv3.bn.bias'\n","Freezing layer 'model.6.m.0.m.0.cv1.conv.weight'\n","Freezing layer 'model.6.m.0.m.0.cv1.bn.weight'\n","Freezing layer 'model.6.m.0.m.0.cv1.bn.bias'\n","Freezing layer 'model.6.m.0.m.0.cv2.conv.weight'\n","Freezing layer 'model.6.m.0.m.0.cv2.bn.weight'\n","Freezing layer 'model.6.m.0.m.0.cv2.bn.bias'\n","Freezing layer 'model.6.m.0.m.1.cv1.conv.weight'\n","Freezing layer 'model.6.m.0.m.1.cv1.bn.weight'\n","Freezing layer 'model.6.m.0.m.1.cv1.bn.bias'\n","Freezing layer 'model.6.m.0.m.1.cv2.conv.weight'\n","Freezing layer 'model.6.m.0.m.1.cv2.bn.weight'\n","Freezing layer 'model.6.m.0.m.1.cv2.bn.bias'\n","Freezing layer 'model.7.conv.weight'\n","Freezing layer 'model.7.bn.weight'\n","Freezing layer 'model.7.bn.bias'\n","Freezing layer 'model.8.cv1.conv.weight'\n","Freezing layer 'model.8.cv1.bn.weight'\n","Freezing layer 'model.8.cv1.bn.bias'\n","Freezing layer 'model.8.cv2.conv.weight'\n","Freezing layer 'model.8.cv2.bn.weight'\n","Freezing layer 'model.8.cv2.bn.bias'\n","Freezing layer 'model.8.m.0.cv1.conv.weight'\n","Freezing layer 'model.8.m.0.cv1.bn.weight'\n","Freezing layer 'model.8.m.0.cv1.bn.bias'\n","Freezing layer 'model.8.m.0.cv2.conv.weight'\n","Freezing layer 'model.8.m.0.cv2.bn.weight'\n","Freezing layer 'model.8.m.0.cv2.bn.bias'\n","Freezing layer 'model.8.m.0.cv3.conv.weight'\n","Freezing layer 'model.8.m.0.cv3.bn.weight'\n","Freezing layer 'model.8.m.0.cv3.bn.bias'\n","Freezing layer 'model.8.m.0.m.0.cv1.conv.weight'\n","Freezing layer 'model.8.m.0.m.0.cv1.bn.weight'\n","Freezing layer 'model.8.m.0.m.0.cv1.bn.bias'\n","Freezing layer 'model.8.m.0.m.0.cv2.conv.weight'\n","Freezing layer 'model.8.m.0.m.0.cv2.bn.weight'\n","Freezing layer 'model.8.m.0.m.0.cv2.bn.bias'\n","Freezing layer 'model.8.m.0.m.1.cv1.conv.weight'\n","Freezing layer 'model.8.m.0.m.1.cv1.bn.weight'\n","Freezing layer 'model.8.m.0.m.1.cv1.bn.bias'\n","Freezing layer 'model.8.m.0.m.1.cv2.conv.weight'\n","Freezing layer 'model.8.m.0.m.1.cv2.bn.weight'\n","Freezing layer 'model.8.m.0.m.1.cv2.bn.bias'\n","Freezing layer 'model.9.cv1.conv.weight'\n","Freezing layer 'model.9.cv1.bn.weight'\n","Freezing layer 'model.9.cv1.bn.bias'\n","Freezing layer 'model.9.cv2.conv.weight'\n","Freezing layer 'model.9.cv2.bn.weight'\n","Freezing layer 'model.9.cv2.bn.bias'\n","Freezing layer 'model.23.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n","\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.4MB 145.7MB/s 0.0s\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 932.0Â±546.8 MB/s, size: 32.2 KB)\n","\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/merged_dataset/train/labels... 6803 images, 1085 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7888/7888 2.1Kit/s 3.8s\n","\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/paper_1_0023_jpg.rf.10fb656afde17ad23be9c7b7c7dcd0b8.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/paper_1_F_0000_jpg.rf.7154e2bb81c73cdd2961d2508b3712a7.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/merged_dataset/train/labels.cache\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 598.2Â±271.9 MB/s, size: 38.6 KB)\n","\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/merged_dataset/valid/labels... 883 images, 4 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 887/887 603.6it/s 1.5s\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/merged_dataset/valid/labels.cache\n","Plotting labels to /content/drive/MyDrive/Intelligence-Test-Models/anticheat_finetuned_seg_v3/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 90 weight(decay=0.0), 101 weight(decay=0.0005), 100 bias(decay=0.0)\n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1m/content/drive/MyDrive/Intelligence-Test-Models/anticheat_finetuned_seg_v3\u001b[0m\n","Starting training for 50 epochs...\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       1/50      2.81G     0.4278     0.5621     0.3325     0.9871         35        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.3it/s 3:35\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 1.1it/s 26.4s\n","                   all        887        977       0.93      0.858       0.88       0.65      0.908      0.839      0.855      0.631\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       2/50      4.13G     0.4462     0.5707     0.3464     0.9942         45        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:18\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.0it/s 14.2s\n","                   all        887        977      0.891      0.867      0.891      0.669      0.914      0.824      0.868      0.643\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       3/50      4.17G     0.4647     0.5579     0.3681      1.001         37        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:17\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.0it/s 14.0s\n","                   all        887        977      0.929      0.842      0.878      0.657      0.916      0.835       0.87       0.65\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       4/50       4.2G     0.4651      0.565     0.3736      1.001         38        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:15\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.0it/s 13.8s\n","                   all        887        977      0.886      0.842      0.884      0.649      0.914      0.799      0.866      0.637\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       5/50      4.24G     0.4636     0.5449     0.3602     0.9974         43        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.6it/s 3:13\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.0it/s 13.8s\n","                   all        887        977      0.895      0.825      0.877      0.653      0.886      0.817      0.868      0.638\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       6/50      4.27G     0.4557     0.5491     0.3567     0.9951         36        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:16\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.5s\n","                   all        887        977       0.91      0.844      0.895      0.671      0.917      0.823      0.884      0.653\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       7/50      4.31G     0.4442     0.5373     0.3449     0.9891         34        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:16\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.0it/s 13.9s\n","                   all        887        977      0.938       0.84       0.91      0.684      0.924      0.828      0.894      0.659\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       8/50      4.34G     0.4435     0.5383     0.3448     0.9892         28        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:16\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.0it/s 13.7s\n","                   all        887        977      0.936       0.83      0.889      0.669      0.877      0.844      0.873      0.647\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       9/50      4.37G     0.4384     0.5078     0.3356     0.9876         40        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:21\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.5s\n","                   all        887        977       0.93      0.861      0.902      0.676      0.912      0.846      0.882      0.652\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      10/50       4.4G     0.4387     0.5087     0.3343     0.9872         41        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:15\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.0it/s 13.9s\n","                   all        887        977      0.892      0.874      0.889      0.672      0.882      0.862      0.876       0.66\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      11/50      4.44G     0.4284     0.5023     0.3218     0.9799         41        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:19\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.0it/s 14.1s\n","                   all        887        977      0.912      0.841      0.886      0.674      0.912      0.815      0.873      0.658\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      12/50      4.47G     0.4279     0.5051     0.3211     0.9779         36        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:16\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.6s\n","                   all        887        977      0.922      0.859      0.896      0.667      0.905      0.837      0.877      0.648\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      13/50       4.5G     0.4263     0.5052     0.3216     0.9797         41        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:17\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.0it/s 14.0s\n","                   all        887        977       0.93      0.857      0.895       0.67      0.887      0.837      0.859      0.649\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      14/50      4.54G     0.4233      0.505      0.314     0.9746         43        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:16\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.0it/s 13.7s\n","                   all        887        977      0.917      0.855      0.895      0.669      0.895      0.834      0.872       0.65\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      15/50      4.57G     0.4195     0.5089     0.3151     0.9779         27        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:21\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.0it/s 13.8s\n","                   all        887        977       0.91       0.87      0.902      0.674      0.929      0.814      0.877      0.655\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      16/50      4.61G     0.4158     0.4889     0.3102     0.9734         43        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:18\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.0it/s 13.8s\n","                   all        887        977      0.879      0.878      0.889      0.668      0.865      0.864      0.873      0.649\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      17/50      4.64G     0.4128     0.4672     0.3045     0.9744         49        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:15\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.0it/s 13.8s\n","                   all        887        977      0.933      0.857      0.899      0.679      0.911      0.836      0.871      0.655\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      18/50      4.67G     0.4113     0.4805     0.3012     0.9703         36        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:14\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.0it/s 13.7s\n","                   all        887        977      0.916      0.863      0.901      0.674      0.885      0.867      0.884      0.653\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      19/50      4.71G     0.4067     0.4753      0.298      0.968         39        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:14\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.5s\n","                   all        887        977      0.918      0.867      0.894      0.674      0.894      0.846      0.863      0.652\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      20/50      4.74G     0.4061     0.4749     0.2985     0.9695         38        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:15\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.6s\n","                   all        887        977      0.911      0.866      0.897      0.662      0.894      0.842      0.868      0.644\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      21/50      4.77G     0.3995     0.4664     0.2906     0.9643         48        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:14\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.0it/s 13.7s\n","                   all        887        977      0.908      0.867      0.896      0.671      0.897      0.841      0.869      0.657\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      22/50      4.81G     0.4017      0.477     0.2923     0.9679         41        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:14\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.0it/s 13.8s\n","                   all        887        977      0.916      0.876      0.906      0.681      0.899      0.859      0.883      0.663\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      23/50      4.84G     0.3957     0.4783     0.2838     0.9647         44        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:16\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.5s\n","                   all        887        977      0.902      0.859      0.883      0.673      0.927      0.836      0.879      0.661\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      24/50      4.87G     0.3951     0.4645     0.2846     0.9625         38        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:14\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.6s\n","                   all        887        977       0.93      0.861      0.896      0.678      0.919      0.849      0.887      0.666\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      25/50      4.91G     0.3964     0.4758     0.2837     0.9638         41        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:17\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.2s\n","                   all        887        977       0.93      0.862      0.901      0.678      0.908      0.836      0.867      0.653\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      26/50      4.94G     0.3933     0.4638     0.2851     0.9634         34        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:13\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.4s\n","                   all        887        977      0.888       0.87      0.889      0.678      0.899      0.832      0.869       0.66\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      27/50      4.97G     0.3844     0.4473     0.2794     0.9614         32        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:14\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.0it/s 13.7s\n","                   all        887        977      0.926      0.874      0.893       0.68      0.905      0.855      0.878       0.66\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      28/50      5.01G     0.3885       0.46     0.2777     0.9605         45        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:14\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.6s\n","                   all        887        977      0.914      0.875        0.9      0.678      0.923      0.835      0.882       0.66\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      29/50      5.04G     0.3835     0.4626     0.2724     0.9579         36        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:14\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.6s\n","                   all        887        977      0.914      0.877      0.906      0.686      0.888      0.863      0.884       0.66\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      30/50      5.07G     0.3787     0.4491     0.2674     0.9561         43        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:16\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.6s\n","                   all        887        977      0.918      0.851      0.892      0.675      0.872       0.85      0.869      0.652\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      31/50      5.11G     0.3825     0.4607      0.271     0.9561         30        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:14\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.4s\n","                   all        887        977      0.918      0.866      0.901      0.681      0.904      0.855      0.884      0.662\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      32/50      5.14G     0.3789     0.4403     0.2665     0.9574         32        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:14\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.3s\n","                   all        887        977      0.924      0.869      0.898      0.683      0.899      0.869      0.885      0.664\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      33/50      5.17G     0.3761     0.4335     0.2631     0.9537         33        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:16\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.0it/s 13.7s\n","                   all        887        977      0.926      0.868      0.899      0.682      0.911      0.855      0.884      0.664\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      34/50      5.21G     0.3718     0.4326     0.2599     0.9539         52        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:14\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.0it/s 14.0s\n","                   all        887        977      0.927      0.866      0.904      0.687      0.903      0.869      0.891      0.668\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      35/50      5.24G     0.3725     0.4364      0.259     0.9527         46        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:16\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.0it/s 13.7s\n","                   all        887        977      0.926      0.868      0.898       0.68      0.909       0.84      0.875       0.66\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      36/50      5.28G     0.3761     0.4468     0.2638     0.9529         45        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:15\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.3s\n","                   all        887        977      0.937      0.869      0.908      0.689      0.928      0.839      0.887      0.665\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      37/50      5.31G     0.3694     0.4308      0.255      0.952         31        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:14\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.3s\n","                   all        887        977      0.944      0.845        0.9      0.681      0.923      0.819       0.87      0.659\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      38/50      5.34G     0.3634     0.4191     0.2524     0.9471         38        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:15\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.5s\n","                   all        887        977      0.945      0.856      0.907      0.679      0.917      0.833      0.877       0.66\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      39/50      5.38G     0.3601     0.4303     0.2497     0.9436         36        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:15\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.0it/s 13.9s\n","                   all        887        977      0.945      0.866      0.902      0.675       0.92      0.845      0.878       0.66\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      40/50      5.41G     0.3588     0.4279     0.2538     0.9435         40        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.5it/s 3:15\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.3s\n","                   all        887        977      0.944      0.869      0.907      0.687      0.931      0.854      0.894      0.665\n","Closing dataloader mosaic\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      41/50      5.45G     0.2851     0.3127     0.1972     0.9217         18        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.7it/s 3:04\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.6s\n","                   all        887        977      0.924      0.877      0.899      0.684      0.911      0.865      0.886      0.664\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      42/50      5.49G     0.2758     0.3037     0.1897     0.9144         17        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.7it/s 3:02\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.6s\n","                   all        887        977      0.942       0.86      0.907      0.682      0.918      0.841      0.881      0.665\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      43/50      5.52G     0.2758     0.3019     0.1874      0.918         13        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.7it/s 3:01\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.3s\n","                   all        887        977      0.934      0.875       0.91      0.685      0.911      0.855      0.884      0.664\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      44/50      5.55G     0.2654     0.2976     0.1776     0.9056         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.7it/s 3:01\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.6s\n","                   all        887        977      0.929       0.87        0.9      0.682      0.909      0.858       0.88       0.66\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      45/50      5.59G     0.2644     0.2954     0.1792     0.9103         15        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.7it/s 3:00\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.4s\n","                   all        887        977      0.939       0.85      0.898      0.684      0.921      0.831      0.877      0.665\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      46/50      5.62G     0.2591     0.2924     0.1761     0.9042         16        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.7it/s 3:01\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.0it/s 13.7s\n","                   all        887        977      0.932      0.857        0.9      0.686      0.903      0.836      0.873      0.661\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      47/50      5.65G     0.2517     0.2869     0.1722      0.899         17        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.7it/s 2:60\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.5s\n","                   all        887        977       0.95      0.851      0.907      0.683       0.92      0.827      0.874      0.663\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      48/50      5.69G     0.2519     0.2885     0.1677     0.8969         16        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.7it/s 3:02\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.4s\n","                   all        887        977      0.934      0.869      0.907      0.688       0.91      0.848      0.882      0.667\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      49/50      5.72G     0.2481     0.2793     0.1664     0.8957         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 493/493 2.8it/s 2:59\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 2.1it/s 13.4s\n","                   all        887        977      0.939      0.858      0.906      0.684      0.925      0.833       0.88      0.667\n","\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 15 epochs. Best results observed at epoch 34, best model saved as best.pt.\n","To update EarlyStopping(patience=15) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n","\n","49 epochs completed in 2.828 hours.\n","Optimizer stripped from /content/drive/MyDrive/Intelligence-Test-Models/anticheat_finetuned_seg_v3/weights/last.pt, 20.5MB\n","Optimizer stripped from /content/drive/MyDrive/Intelligence-Test-Models/anticheat_finetuned_seg_v3/weights/best.pt, 20.5MB\n","\n","Validating /content/drive/MyDrive/Intelligence-Test-Models/anticheat_finetuned_seg_v3/weights/best.pt...\n","Ultralytics 8.3.237 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n","YOLO11s-seg summary (fused): 113 layers, 10,068,364 parameters, 0 gradients, 32.8 GFLOPs\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 1.7it/s 16.7s\n","                   all        887        977      0.927      0.866      0.904      0.687      0.903      0.869      0.891      0.669\n","                 phone        295        362      0.893      0.828      0.884      0.668      0.868      0.835      0.877      0.643\n","              material        469        474      0.986      0.998      0.994      0.988      0.985      0.998      0.994      0.986\n","            headphones        119        141      0.901      0.773      0.835      0.405      0.857      0.773      0.802      0.377\n","Speed: 0.2ms preprocess, 6.2ms inference, 0.0ms loss, 3.7ms postprocess per image\n","Results saved to \u001b[1m/content/drive/MyDrive/Intelligence-Test-Models/anticheat_finetuned_seg_v3\u001b[0m\n","\n","âœ… Training completed!\n"]}]},{"cell_type":"code","source":["# Export to ONNX\n","BEST_MODEL_PATH = '/content/drive/MyDrive/Intelligence-Test-Models/anticheat_finetuned_seg_v3/weights/best.pt'\n","\n","model = YOLO(BEST_MODEL_PATH)\n","\n","print(\"ğŸ“¦ Exporting segmentation model to ONNX...\")\n","model.export(\n","    format='onnx',\n","    imgsz=640,\n","    simplify=True,\n","    dynamic=False,\n","    opset=17\n",")\n","\n","onnx_path = BEST_MODEL_PATH.replace('.pt', '.onnx')\n","print(f\"\\nâœ… ONNX model saved to: {onnx_path}\")"],"metadata":{"id":"export","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765802662883,"user_tz":-420,"elapsed":14913,"user":{"displayName":"Pháº¡m Ngá»c Huy HoÃ n","userId":"05738996591361131973"}},"outputId":"1dad3551-40b2-432f-d06b-a061cb5d2fb2"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ“¦ Exporting segmentation model to ONNX...\n","Ultralytics 8.3.237 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CPU (Intel Xeon CPU @ 2.00GHz)\n","ğŸ’¡ ProTip: Export to OpenVINO format for best performance on Intel hardware. Learn more at https://docs.ultralytics.com/integrations/openvino/\n","YOLO11s-seg summary (fused): 113 layers, 10,068,364 parameters, 0 gradients, 32.8 GFLOPs\n","\n","\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/content/drive/MyDrive/Intelligence-Test-Models/anticheat_finetuned_seg_v3/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) ((1, 40, 8400), (1, 32, 160, 160)) (19.6 MB)\n","\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnx>=1.12.0,<=1.19.1', 'onnxslim>=0.1.71', 'onnxruntime-gpu'] not found, attempting AutoUpdate...\n","Using Python 3.12.12 environment at: /usr\n","Resolved 14 packages in 183ms\n","Prepared 4 packages in 8.12s\n","Uninstalled 1 package in 981ms\n","Installed 4 packages in 440ms\n"," + colorama==0.4.6\n"," - onnx==1.20.0\n"," + onnx==1.19.1\n"," + onnxruntime-gpu==1.23.2\n"," + onnxslim==0.1.80\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success âœ… 10.4s\n","WARNING âš ï¸ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","\n","\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.19.1 opset 17...\n","\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.80...\n","\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 13.6s, saved as '/content/drive/MyDrive/Intelligence-Test-Models/anticheat_finetuned_seg_v3/weights/best.onnx' (38.7 MB)\n","\n","Export complete (14.8s)\n","Results saved to \u001b[1m/content/drive/MyDrive/Intelligence-Test-Models/anticheat_finetuned_seg_v3/weights\u001b[0m\n","Predict:         yolo predict task=segment model=/content/drive/MyDrive/Intelligence-Test-Models/anticheat_finetuned_seg_v3/weights/best.onnx imgsz=640  \n","Validate:        yolo val task=segment model=/content/drive/MyDrive/Intelligence-Test-Models/anticheat_finetuned_seg_v3/weights/best.onnx imgsz=640 data=/content/merged_dataset/data.yaml  \n","Visualize:       https://netron.app\n","\n","âœ… ONNX model saved to: /content/drive/MyDrive/Intelligence-Test-Models/anticheat_finetuned_seg_v3/weights/best.onnx\n"]}]},{"cell_type":"code","source":["# Test the new model\n","import onnxruntime as ort\n","from PIL import Image, ImageDraw\n","\n","session = ort.InferenceSession(onnx_path)\n","input_name = session.get_inputs()[0].name\n","classes = ['person', 'phone', 'material', 'headphones']\n","\n","def test_image(name, img):\n","    img = img.convert('RGB').resize((640, 640))\n","    img_array = np.array(img).astype(np.float32) / 255.0\n","    img_array = np.transpose(img_array, (2, 0, 1))\n","    img_array = np.expand_dims(img_array, axis=0)\n","\n","    outputs = session.run(None, {input_name: img_array})\n","    # Segmentation models output: (detection_output, mask_output)\n","    output = outputs[0]\n","    class_scores = output[0, 4:8, :]\n","\n","    print(f\"\\n{name}:\")\n","    for i, cls in enumerate(classes):\n","        max_score = class_scores[i].max()\n","        print(f\"  {cls}: max={max_score:.4f}\")\n","\n","# Test with phone simulation\n","phone_img = Image.new('RGB', (640, 640), color=(200, 200, 200))\n","draw = ImageDraw.Draw(phone_img)\n","draw.rectangle([250, 200, 390, 450], fill=(20, 20, 20))\n","draw.rectangle([260, 210, 380, 440], fill=(50, 50, 80))\n","test_image(\"Phone simulation\", phone_img)\n","\n","# Test with headphones simulation\n","headphones_img = Image.new('RGB', (640, 640), color=(200, 200, 200))\n","draw = ImageDraw.Draw(headphones_img)\n","draw.arc([200, 150, 440, 350], 180, 0, fill=(30, 30, 30), width=20)\n","draw.ellipse([180, 280, 260, 400], fill=(40, 40, 40))\n","draw.ellipse([380, 280, 460, 400], fill=(40, 40, 40))\n","test_image(\"Headphones simulation\", headphones_img)\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"If phone scores improved (>0.1), the fine-tuning worked!\")\n","print(\"=\"*50)"],"metadata":{"id":"test","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765802710133,"user_tz":-420,"elapsed":1588,"user":{"displayName":"Pháº¡m Ngá»c Huy HoÃ n","userId":"05738996591361131973"}},"outputId":"aac086ec-dccf-48a8-dfea-87febfb231b3"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Phone simulation:\n","  person: max=0.0000\n","  phone: max=0.0228\n","  material: max=0.0001\n","  headphones: max=0.0001\n","\n","Headphones simulation:\n","  person: max=0.0000\n","  phone: max=0.0003\n","  material: max=0.0000\n","  headphones: max=0.0006\n","\n","==================================================\n","If phone scores improved (>0.1), the fine-tuning worked!\n","==================================================\n"]}]},{"cell_type":"code","source":["# Download the ONNX file\n","print(\"\\nğŸ“‹ NEXT STEPS:\")\n","print(\"1. Download the ONNX file from Google Drive\")\n","print(f\"   Location: {onnx_path}\")\n","print(\"2. Rename it to: anticheat_yolo11s.onnx\")\n","print(\"3. Copy to: Intelligence-Test/public/models/\")\n","print(\"4. Rebuild and deploy the web app\")\n","print(\"\\nğŸ‰ Done!\")"],"metadata":{"id":"finish","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765802749315,"user_tz":-420,"elapsed":50,"user":{"displayName":"Pháº¡m Ngá»c Huy HoÃ n","userId":"05738996591361131973"}},"outputId":"718ae647-59d2-4195-9d06-e2a3f0c37849"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸ“‹ NEXT STEPS:\n","1. Download the ONNX file from Google Drive\n","   Location: /content/drive/MyDrive/Intelligence-Test-Models/anticheat_finetuned_seg_v3/weights/best.onnx\n","2. Rename it to: anticheat_yolo11s.onnx\n","3. Copy to: Intelligence-Test/public/models/\n","4. Rebuild and deploy the web app\n","\n","ğŸ‰ Done!\n"]}]}]}