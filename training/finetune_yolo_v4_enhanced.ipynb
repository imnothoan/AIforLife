{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üéØ YOLO Segmentation Anti-Cheat Model Training v4\n",
        "\n",
        "**UPGRADED VERSION** with more diverse datasets for improved detection!\n",
        "\n",
        "## Target Classes:\n",
        "- üì± **Phone** - Multiple angles, in-hand, on-desk, hidden\n",
        "- üìÑ **Material/Paper/Book/Notebook** - Cheat sheets, notes, textbooks\n",
        "- üë§ **Person** - For multi-person detection\n",
        "- üéß **Headphones** - Earbuds, AirPods, wireless, wired, headsets\n",
        "\n",
        "## New Features in v4:\n",
        "- More segmentation datasets from Roboflow Universe\n",
        "- Better class mapping for variations\n",
        "- Improved data augmentation\n",
        "- Higher resolution training option\n",
        "\n",
        "## Instructions:\n",
        "1. Mount Google Drive\n",
        "2. Run all cells in order\n",
        "3. Download the new ONNX file when done\n",
        "4. Replace `Intelligence-Test/public/models/anticheat_yolo11s.onnx`"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mount_drive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install ultralytics -q\n",
        "!pip install onnx onnxruntime -q\n",
        "!pip install roboflow -q\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from ultralytics import YOLO\n",
        "\n",
        "print(\"‚úÖ Dependencies installed!\")"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration - EXPANDED DATASETS\n",
        "TARGET_CLASSES = ['person', 'phone', 'material', 'headphones']\n",
        "\n",
        "# EXPANDED Dataset URLs from Roboflow Universe\n",
        "# Prioritized by class weakness and diversity\n",
        "DATASETS = [\n",
        "    # =============================================\n",
        "    # PHONE DATASETS - HIGH PRIORITY (many angles)\n",
        "    # =============================================\n",
        "    (\"phone_1\", \"https://app.roboflow.com/ds/5ReObgnLbQ?key=HTPSgVzDLW\"),\n",
        "    (\"phone_2\", \"https://app.roboflow.com/ds/f9k54F7Azq?key=eYssUekSYc\"),\n",
        "    # Phone detection in various environments\n",
        "    (\"phone_in_hand\", \"https://universe.roboflow.com/ds/H1dXqnAb7N?key=JFqQMFoL1k\"),\n",
        "    # Smartphone dataset\n",
        "    (\"smartphone\", \"https://universe.roboflow.com/ds/2knGg7q8NP?key=pPnGfqSkOw\"),\n",
        "    \n",
        "    # =============================================\n",
        "    # PAPER/BOOK/MATERIAL DATASETS\n",
        "    # =============================================\n",
        "    (\"paper_1\", \"https://app.roboflow.com/ds/inuabMtp6t?key=jbu7HTlrBf\"),\n",
        "    (\"paper_2\", \"https://app.roboflow.com/ds/b4oxAhlW40?key=4A761Kjm5F\"),\n",
        "    # Book dataset\n",
        "    (\"book_1\", \"https://universe.roboflow.com/ds/krnm1ZGr5g?key=jQELvT3SXU\"),\n",
        "    # Document/Paper detection\n",
        "    (\"document\", \"https://universe.roboflow.com/ds/jxG2VGLyQT?key=qwz3k1Nv5F\"),\n",
        "    \n",
        "    # =============================================\n",
        "    # HEADPHONES/EARBUDS DATASETS (various types)\n",
        "    # =============================================\n",
        "    (\"headphones_1\", \"https://app.roboflow.com/ds/qqqEeSKAlk?key=GT1Xa65onI\"),\n",
        "    (\"headphones_2\", \"https://app.roboflow.com/ds/cKHwOqmuda?key=qL10KsWlBt\"),\n",
        "    # Earbuds/AirPods specific\n",
        "    (\"earbuds\", \"https://universe.roboflow.com/ds/wKj4kR2qgM?key=8TFdpL5nQV\"),\n",
        "    # Ear device detection\n",
        "    (\"ear_device\", \"https://universe.roboflow.com/ds/yLmKp3QR7N?key=FxHjVtB8sL\"),\n",
        "    \n",
        "    # =============================================\n",
        "    # PERSON DATASETS (for multi-person detection)\n",
        "    # =============================================\n",
        "    (\"person_1\", \"https://app.roboflow.com/ds/PwRwV0c1jL?key=FgXbXeqlpH\"),\n",
        "    # People detection\n",
        "    (\"person_coco\", \"https://universe.roboflow.com/ds/tGq8jL5rNM?key=Kw2VhR9pXs\"),\n",
        "]\n",
        "\n",
        "# EXPANDED Class mapping - Cover more variations\n",
        "CLASS_MAPPING = {\n",
        "    # Person variations\n",
        "    'person': 'person', 'student': 'person', 'face': 'person', 'head': 'person',\n",
        "    'human': 'person', 'people': 'person', 'man': 'person', 'woman': 'person',\n",
        "    'boy': 'person', 'girl': 'person', 'adult': 'person', 'child': 'person',\n",
        "    \n",
        "    # Phone variations\n",
        "    'phone': 'phone', 'mobile': 'phone', 'cell phone': 'phone',\n",
        "    'telephone': 'phone', 'smartphone': 'phone', 'cellphone': 'phone',\n",
        "    'mobile phone': 'phone', 'iphone': 'phone', 'android': 'phone',\n",
        "    'mobile_phone': 'phone', 'cell_phone': 'phone', 'smart_phone': 'phone',\n",
        "    'handphone': 'phone', 'hp': 'phone', 'device': 'phone',\n",
        "    'ProductRecog - v2 2024-11-05 7-03am': 'phone',\n",
        "    'ProductRecog - v2 2024-11-05 7:03am': 'phone',\n",
        "    \n",
        "    # Material variations\n",
        "    'paper': 'material', 'document': 'material', 'book': 'material',\n",
        "    'notebook': 'material', 'notes': 'material', 'sheet': 'material',\n",
        "    'material': 'material', 'cheat sheet': 'material', 'PAPER': 'material',\n",
        "    'Paper': 'material', 'textbook': 'material', 'magazine': 'material',\n",
        "    'newspaper': 'material', 'letter': 'material', 'card': 'material',\n",
        "    'note': 'material', 'page': 'material', 'papers': 'material',\n",
        "    'books': 'material', 'reading': 'material', 'text': 'material',\n",
        "    \n",
        "    # Headphones variations\n",
        "    'headphone': 'headphones', 'headphones': 'headphones',\n",
        "    'earphone': 'headphones', 'earphones': 'headphones',\n",
        "    'headset': 'headphones', 'earbuds': 'headphones', 'earbud': 'headphones',\n",
        "    'airpods': 'headphones', 'ear device': 'headphones', 'Headphone': 'headphones',\n",
        "    'left earbud': 'headphones', 'eardevice': 'headphones',\n",
        "    'right earbud': 'headphones', 'ear_device': 'headphones',\n",
        "    'wireless_earbuds': 'headphones', 'bluetooth_headphones': 'headphones',\n",
        "    'ear': 'headphones', 'airpod': 'headphones', 'headphone_on': 'headphones',\n",
        "}\n",
        "\n",
        "def normalize_class(class_name):\n",
        "    \"\"\"Map source class name to target class index\"\"\"\n",
        "    class_name_lower = class_name.lower().strip().replace('-', ' ').replace('_', ' ')\n",
        "    for key, target in CLASS_MAPPING.items():\n",
        "        if key.lower() == class_name_lower:\n",
        "            return TARGET_CLASSES.index(target)\n",
        "    # Partial matching for flexibility\n",
        "    for key, target in CLASS_MAPPING.items():\n",
        "        if key.lower() in class_name_lower or class_name_lower in key.lower():\n",
        "            return TARGET_CLASSES.index(target)\n",
        "    return -1\n",
        "\n",
        "def bbox_to_segment(bbox_coords):\n",
        "    \"\"\"\n",
        "    Convert YOLO bounding box to segmentation polygon format.\n",
        "    Required for training segmentation models with detection datasets.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        xc, yc, w, h = map(float, bbox_coords)\n",
        "        if not (0 <= xc <= 1 and 0 <= yc <= 1 and 0 < w <= 1 and 0 < h <= 1):\n",
        "            return None\n",
        "        x1, y1 = max(0, xc - w/2), max(0, yc - h/2)\n",
        "        x2, y2 = min(1, xc + w/2), max(0, yc - h/2)\n",
        "        x3, y3 = min(1, xc + w/2), min(1, yc + h/2)\n",
        "        x4, y4 = max(0, xc - w/2), min(1, yc + h/2)\n",
        "        return f\"{x1} {y1} {x2} {y2} {x3} {y3} {x4} {y4}\"\n",
        "    except (ValueError, TypeError):\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ Configuration loaded!\")\n",
        "print(f\"Target classes: {TARGET_CLASSES}\")\n",
        "print(f\"Total datasets: {len(DATASETS)}\")"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download datasets with error handling\n",
        "!mkdir -p /content/raw_datasets\n",
        "%cd /content/raw_datasets\n",
        "\n",
        "successful = []\n",
        "failed = []\n",
        "\n",
        "for name, url in DATASETS:\n",
        "    if os.path.exists(name):\n",
        "        print(f\"‚úì {name} already exists\")\n",
        "        successful.append(name)\n",
        "        continue\n",
        "    \n",
        "    print(f\"üì• Downloading {name}...\")\n",
        "    try:\n",
        "        os.makedirs(name, exist_ok=True)\n",
        "        !curl -L \"{url}\" > {name}/dataset.zip 2>/dev/null\n",
        "        \n",
        "        # Check if download was successful\n",
        "        zip_path = f\"{name}/dataset.zip\"\n",
        "        if os.path.exists(zip_path) and os.path.getsize(zip_path) > 1000:\n",
        "            !unzip -q {name}/dataset.zip -d {name} 2>/dev/null || true\n",
        "            !rm -f {name}/dataset.zip\n",
        "            successful.append(name)\n",
        "            print(f\"   ‚úì Downloaded successfully\")\n",
        "        else:\n",
        "            failed.append(name)\n",
        "            print(f\"   ‚úó Download failed or empty file\")\n",
        "            !rm -rf {name}\n",
        "    except Exception as e:\n",
        "        failed.append(name)\n",
        "        print(f\"   ‚úó Error: {e}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Downloaded: {len(successful)} datasets\")\n",
        "if failed:\n",
        "    print(f\"‚ö†Ô∏è Failed: {len(failed)} datasets - {failed}\")"
      ],
      "metadata": {
        "id": "download_datasets"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge and convert datasets to SEGMENTATION format\n",
        "OUTPUT_DIR = '/content/merged_dataset_v4'\n",
        "\n",
        "!rm -rf {OUTPUT_DIR}\n",
        "os.makedirs(f\"{OUTPUT_DIR}/train/images\", exist_ok=True)\n",
        "os.makedirs(f\"{OUTPUT_DIR}/train/labels\", exist_ok=True)\n",
        "os.makedirs(f\"{OUTPUT_DIR}/valid/images\", exist_ok=True)\n",
        "os.makedirs(f\"{OUTPUT_DIR}/valid/labels\", exist_ok=True)\n",
        "\n",
        "stats = {'train': 0, 'valid': 0, 'bbox_converted': 0, 'seg_preserved': 0}\n",
        "class_counts = {c: 0 for c in TARGET_CLASSES}\n",
        "\n",
        "for name, _ in DATASETS:\n",
        "    dataset_dir = f\"/content/raw_datasets/{name}\"\n",
        "    if not os.path.exists(dataset_dir):\n",
        "        continue\n",
        "\n",
        "    # Find data.yaml\n",
        "    data_yaml = None\n",
        "    for root, dirs, files in os.walk(dataset_dir):\n",
        "        if 'data.yaml' in files:\n",
        "            data_yaml = os.path.join(root, 'data.yaml')\n",
        "            break\n",
        "\n",
        "    if not data_yaml:\n",
        "        print(f\"‚ö†Ô∏è No data.yaml in {name}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        with open(data_yaml, 'r') as f:\n",
        "            config = yaml.safe_load(f)\n",
        "    except:\n",
        "        print(f\"‚ö†Ô∏è Cannot parse data.yaml in {name}\")\n",
        "        continue\n",
        "\n",
        "    source_classes = config.get('names', [])\n",
        "    if isinstance(source_classes, dict):\n",
        "        source_classes = list(source_classes.values())\n",
        "\n",
        "    print(f\"\\nüìÇ Processing {name}...\")\n",
        "    print(f\"   Classes: {source_classes}\")\n",
        "\n",
        "    for split in ['train', 'valid', 'test']:\n",
        "        img_dir = None\n",
        "        lbl_dir = None\n",
        "\n",
        "        for try_path in [dataset_dir, os.path.dirname(data_yaml)]:\n",
        "            if os.path.exists(os.path.join(try_path, split, 'images')):\n",
        "                img_dir = os.path.join(try_path, split, 'images')\n",
        "                lbl_dir = os.path.join(try_path, split, 'labels')\n",
        "                break\n",
        "\n",
        "        if not img_dir or not os.path.exists(img_dir):\n",
        "            continue\n",
        "\n",
        "        out_split = 'train' if split in ['train', 'test'] else 'valid'\n",
        "        count = 0\n",
        "\n",
        "        for img_file in os.listdir(img_dir):\n",
        "            if not img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                continue\n",
        "\n",
        "            src_img = os.path.join(img_dir, img_file)\n",
        "            dst_img = os.path.join(OUTPUT_DIR, out_split, 'images', f\"{name}_{img_file}\")\n",
        "            shutil.copy(src_img, dst_img)\n",
        "\n",
        "            lbl_file = os.path.splitext(img_file)[0] + '.txt'\n",
        "            src_lbl = os.path.join(lbl_dir, lbl_file)\n",
        "            dst_lbl = os.path.join(OUTPUT_DIR, out_split, 'labels', f\"{name}_{lbl_file}\")\n",
        "\n",
        "            if os.path.exists(src_lbl):\n",
        "                with open(src_lbl, 'r') as f:\n",
        "                    lines = f.readlines()\n",
        "\n",
        "                new_lines = []\n",
        "                for line in lines:\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) < 5:\n",
        "                        continue\n",
        "\n",
        "                    try:\n",
        "                        old_class_id = int(parts[0])\n",
        "                    except:\n",
        "                        continue\n",
        "                        \n",
        "                    if old_class_id < len(source_classes):\n",
        "                        old_class_name = source_classes[old_class_id]\n",
        "                        new_class_id = normalize_class(old_class_name)\n",
        "\n",
        "                        if new_class_id >= 0:\n",
        "                            class_counts[TARGET_CLASSES[new_class_id]] += 1\n",
        "                            \n",
        "                            if len(parts) == 5:\n",
        "                                segment_coords = bbox_to_segment(parts[1:])\n",
        "                                if segment_coords:\n",
        "                                    new_lines.append(f\"{new_class_id} {segment_coords}\")\n",
        "                                    stats['bbox_converted'] += 1\n",
        "                            elif len(parts) >= 9:\n",
        "                                new_lines.append(f\"{new_class_id} {' '.join(parts[1:])}\")\n",
        "                                stats['seg_preserved'] += 1\n",
        "\n",
        "                if new_lines:\n",
        "                    with open(dst_lbl, 'w') as f:\n",
        "                        f.write('\\n'.join(new_lines))\n",
        "                    count += 1\n",
        "\n",
        "        stats[out_split] += count\n",
        "        if count > 0:\n",
        "            print(f\"   {split} -> {out_split}: {count} images\")\n",
        "\n",
        "# Create data.yaml\n",
        "data_yaml_content = {\n",
        "    'path': OUTPUT_DIR,\n",
        "    'train': 'train/images',\n",
        "    'val': 'valid/images',\n",
        "    'names': {i: name for i, name in enumerate(TARGET_CLASSES)},\n",
        "    'nc': len(TARGET_CLASSES),\n",
        "}\n",
        "\n",
        "with open(f\"{OUTPUT_DIR}/data.yaml\", 'w') as f:\n",
        "    yaml.dump(data_yaml_content, f, default_flow_style=False)\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(f\"‚úÖ Dataset prepared for SEGMENTATION training!\")\n",
        "print(f\"   Train images: {stats['train']}\")\n",
        "print(f\"   Valid images: {stats['valid']}\")\n",
        "print(f\"\\nüìä Class distribution:\")\n",
        "for cls, count in class_counts.items():\n",
        "    print(f\"   {cls}: {count} instances\")\n",
        "print(f\"\\nüìä Label conversion:\")\n",
        "print(f\"   Bounding boxes -> polygons: {stats['bbox_converted']}\")\n",
        "print(f\"   Segmentation preserved: {stats['seg_preserved']}\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "merge_datasets"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Configuration\n",
        "# Option 1: Continue from previous best model (recommended for fine-tuning)\n",
        "BASE_MODEL_PATH = '/content/drive/MyDrive/Intelligence-Test-Models/anticheat_finetuned_seg_v3/weights/best.pt'\n",
        "\n",
        "# Option 2: Start fresh from pretrained YOLO11 segmentation model\n",
        "# BASE_MODEL_PATH = 'yolo11s-seg.pt'  # Small and fast\n",
        "# BASE_MODEL_PATH = 'yolo11m-seg.pt'  # Medium\n",
        "# BASE_MODEL_PATH = 'yolo11x-seg.pt'  # Large and accurate\n",
        "\n",
        "print(f\"Base model: {BASE_MODEL_PATH}\")\n",
        "if os.path.exists(BASE_MODEL_PATH):\n",
        "    print(\"‚úÖ Model file found!\")\n",
        "else:\n",
        "    if BASE_MODEL_PATH.startswith('yolo'):\n",
        "        print(f\"üì• Will download pretrained model: {BASE_MODEL_PATH}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Model file not found - will use pretrained yolo11s-seg.pt\")\n",
        "        BASE_MODEL_PATH = 'yolo11s-seg.pt'"
      ],
      "metadata": {
        "id": "setup_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training with enhanced settings\n",
        "model = YOLO(BASE_MODEL_PATH)\n",
        "\n",
        "print(\"üöÄ Starting training...\")\n",
        "print(\"This may take 1-2 hours depending on dataset size.\")\n",
        "\n",
        "results = model.train(\n",
        "    data=f\"{OUTPUT_DIR}/data.yaml\",\n",
        "    epochs=100,             # More epochs for better convergence\n",
        "    imgsz=640,              # Standard size\n",
        "    batch=16,               # Adjust based on GPU memory\n",
        "    patience=20,            # Early stopping patience\n",
        "    lr0=0.0005,             # Lower LR for fine-tuning\n",
        "    lrf=0.01,               # Final LR factor\n",
        "    warmup_epochs=5,        # Longer warmup\n",
        "    freeze=5,               # Freeze fewer layers for more learning\n",
        "    augment=True,           # Enable augmentation\n",
        "    mosaic=1.0,             # Mosaic augmentation\n",
        "    mixup=0.1,              # MixUp augmentation\n",
        "    copy_paste=0.1,         # Copy-paste augmentation\n",
        "    degrees=10,             # Rotation augmentation\n",
        "    translate=0.1,          # Translation\n",
        "    scale=0.5,              # Scale variation\n",
        "    shear=2.0,              # Shear\n",
        "    perspective=0.0001,     # Perspective\n",
        "    flipud=0.0,             # No vertical flip\n",
        "    fliplr=0.5,             # Horizontal flip\n",
        "    project='/content/drive/MyDrive/Intelligence-Test-Models',\n",
        "    name='anticheat_finetuned_seg_v4',\n",
        "    exist_ok=True,\n",
        "    device=0,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Training completed!\")"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export to ONNX\n",
        "import glob\n",
        "\n",
        "# Find the best model\n",
        "model_dirs = glob.glob('/content/drive/MyDrive/Intelligence-Test-Models/anticheat_finetuned_seg_v4*/weights/best.pt')\n",
        "if model_dirs:\n",
        "    BEST_MODEL_PATH = sorted(model_dirs)[-1]  # Get latest\n",
        "else:\n",
        "    BEST_MODEL_PATH = '/content/drive/MyDrive/Intelligence-Test-Models/anticheat_finetuned_seg_v4/weights/best.pt'\n",
        "\n",
        "print(f\"Loading model: {BEST_MODEL_PATH}\")\n",
        "model = YOLO(BEST_MODEL_PATH)\n",
        "\n",
        "print(\"üì¶ Exporting to ONNX...\")\n",
        "model.export(\n",
        "    format='onnx',\n",
        "    imgsz=640,\n",
        "    simplify=True,\n",
        "    dynamic=False,\n",
        "    opset=17\n",
        ")\n",
        "\n",
        "onnx_path = BEST_MODEL_PATH.replace('.pt', '.onnx')\n",
        "print(f\"\\n‚úÖ ONNX model saved: {onnx_path}\")"
      ],
      "metadata": {
        "id": "export"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test and validate\n",
        "print(\"\\nüìã VALIDATION RESULTS:\")\n",
        "model = YOLO(BEST_MODEL_PATH)\n",
        "metrics = model.val(data=f\"{OUTPUT_DIR}/data.yaml\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä FINAL METRICS:\")\n",
        "print(f\"   Overall mAP50: {metrics.box.map50:.3f}\")\n",
        "print(f\"   Overall mAP50-95: {metrics.box.map:.3f}\")\n",
        "print(\"\\nPer-class mAP50:\")\n",
        "for i, cls in enumerate(TARGET_CLASSES):\n",
        "    if i < len(metrics.box.ap50):\n",
        "        print(f\"   {cls}: {metrics.box.ap50[i]:.3f}\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "validate"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final instructions\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìã NEXT STEPS:\")\n",
        "print(\"=\"*60)\n",
        "print(\"1. Download ONNX file from Google Drive:\")\n",
        "print(f\"   {onnx_path}\")\n",
        "print(\"\\n2. Rename to: anticheat_yolo11s.onnx\")\n",
        "print(\"\\n3. Copy to your project:\")\n",
        "print(\"   Intelligence-Test/public/models/anticheat_yolo11s.onnx\")\n",
        "print(\"\\n4. Rebuild the web app:\")\n",
        "print(\"   cd Intelligence-Test && npm run build\")\n",
        "print(\"\\n5. Deploy!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nüéâ Training complete! Good luck with your exams!\")"
      ],
      "metadata": {
        "id": "finish"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
